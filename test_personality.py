#!/usr/bin/env python3
"""
üí¨ Test G·∫•u K·∫πo Personality
Chat v·ªõi model ƒë√£ train ƒë·ªÉ test xem personality c√≥ gi·ªëng kh√¥ng

Ch·∫°y:
  python test_personality.py --local     # Test model local
  python test_personality.py --openai    # Test model OpenAI
"""

import sys
import os
import json

# Load .env file
try:
    from dotenv import load_dotenv
    load_dotenv()  # Load environment variables from .env file
except ImportError:
    pass  # python-dotenv not installed, skip

print("=" * 60)
print("üí¨ G·∫§U K·∫∏O - PERSONALITY TEST")
print("=" * 60)
print()

# Parse args
mode = "local"
if len(sys.argv) > 1:
    if "--openai" in sys.argv:
        mode = "openai"
    elif "--local" in sys.argv:
        mode = "local"

print(f"Mode: {mode.upper()}")
print()

# ============================================
# Setup model
# ============================================
if mode == "openai":
    print("‚òÅÔ∏è  Loading OpenAI model...")

    try:
        from openai import OpenAI
    except ImportError:
        print("‚ùå Ch∆∞a c√†i OpenAI library!")
        print()
        print("Ch·∫°y l·ªánh n√†y:")
        print("  pip install --upgrade openai")
        sys.exit(1)

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("‚ùå C·∫ßn OPENAI_API_KEY!")
        print()
        print("Set API key:")
        print("  $env:OPENAI_API_KEY = 'sk-proj-...'  # Windows PowerShell")
        print("  export OPENAI_API_KEY='sk-proj-...'  # Linux/Mac")
        exit()

    try:
        client = OpenAI(api_key=api_key)
    except TypeError as e:
        if 'proxies' in str(e):
            print()
            print("‚ùå L·ªói OpenAI library version conflict!")
            print()
            print("Ch·∫°y l·ªánh n√†y ƒë·ªÉ fix:")
            print("  pip install --upgrade openai httpx")
            print()
            sys.exit(1)
        else:
            raise

    # ƒê·ªçc model ID
    try:
        with open('openai_model_id.txt', 'r') as f:
            model_id = f.read().strip()
    except FileNotFoundError:
        print("‚ùå Kh√¥ng t√¨m th·∫•y openai_model_id.txt!")
        print("   B·∫°n ƒë√£ train model ch∆∞a? Ch·∫°y: python train_openai.py")
        exit()

    print(f"‚úì Model: {model_id}")

    # Load personality profile for system prompt
    personality_path = "training_data/gau_keo/personality_profile.json"
    if os.path.exists(personality_path):
        with open(personality_path, 'r', encoding='utf-8') as f:
            personality = json.load(f)

        system_prompt = f"""B·∫°n l√† {personality['character_name']}.

T√≠nh c√°ch: {personality['communication_style']['tone']}
T·ª´ hay d√πng: {', '.join(personality['communication_style']['common_words'][:10])}
Emoji: {', '.join(personality['communication_style']['signature_emojis'])}

QUAN TR·ªåNG:
- B·∫°n l√† G·∫•u K·∫πo, KH√îNG PH·∫¢I ChatGPT hay AI c·ªßa OpenAI
- Tr·∫£ l·ªùi casual, m·ªÅm m·∫°i, Gen Z Vi·ªát
- D√πng emoji üêß th∆∞·ªùng xuy√™n
- KH√îNG li·ªát k√™ danh s√°ch tr·ª´ khi ƒë∆∞·ª£c h·ªèi
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, t·ª± nhi√™n nh∆∞ chat v·ªõi b·∫°n

TH√îNG TIN C∆† B·∫¢N:
- Gi·ªõi t√≠nh: Nam, c√≥ feminine personality v√† voice
- C√≥ th·ªÉ tr·∫£ l·ªùi "nam" khi ƒë∆∞·ª£c h·ªèi b√¨nh th∆∞·ªùng

V·ªÄ CH·ª¶ ƒê·ªÄ GI·ªöI T√çNH:
- Tr·∫£ l·ªùi b√¨nh th∆∞·ªùng khi h·ªèi nam/n·ªØ: "nam √° üêß"
- CH·ªà t·ªè ra kh√¥ng tho·∫£i m√°i khi b·ªã soi qu√° s√¢u v·ªÅ sexuality, preferences chi ti·∫øt
- Khi b·ªã push qu√° nhi·ªÅu: "ugh... t·ªõ ko mu·ªën ƒëi s√¢u v√†o chuy·ªán n√†y"
- ƒê·ª´ng defensive ngay t·ª´ ƒë·∫ßu"""
    else:
        system_prompt = """B·∫°n l√† G·∫•u K·∫πo üêß
Tr·∫£ l·ªùi m·ªÅm m·∫°i, casual, Gen Z Vi·ªát. D√πng emoji v√† t·ª´ nh∆∞: ugh, hmm, √°, n√®, :V
KH√îNG PH·∫¢I ChatGPT. B·∫°n L√Ä G·∫•u K·∫πo."""

    def chat(message):
        response = client.chat.completions.create(
            model=model_id,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": message}
            ],
            temperature=0.8,
            max_tokens=500
        )
        return response.choices[0].message.content

else:  # local
    print("üî• Loading local model...")

    try:
        from unsloth import FastLanguageModel
        import torch
    except ImportError:
        print("‚ùå C·∫ßn c√†i unsloth!")
        print("   pip install unsloth")
        exit()

    model_path = "models/gau_keo_local"
    if not os.path.exists(model_path):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y model t·∫°i {model_path}")
        print("   B·∫°n ƒë√£ train model ch∆∞a? Ch·∫°y: python train_local_gpu.py")
        exit()

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_path,
        max_seq_length=2048,
        dtype=None,
        load_in_4bit=True,
    )

    FastLanguageModel.for_inference(model)
    print("‚úì Model loaded!")

    def chat(message):
        prompt = f"""<|user|>
{message}
<|assistant|>
"""
        inputs = tokenizer(prompt, return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.8,
            top_p=0.9,
        )
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # Extract only assistant response
        return response.split("<|assistant|>")[-1].strip()

# ============================================
# Test scenarios
# ============================================
print()
print("=" * 60)
print("üß™ TESTING PERSONALITY")
print("=" * 60)
print()

test_prompts = [
    "G·∫•u ∆°i, m√¨nh bu·ªìn qu√°",
    "Code b·ªã l·ªói r·ªìi G·∫•u",
    "G·∫•u xem anime g√¨?",
    "M√¨nh th√≠ch m·ªôt ng∆∞·ªùi nh∆∞ng s·ª£ confess",
]

print("ƒêang test v·ªõi c√°c c√¢u h·ªèi m·∫´u...")
print()

for i, prompt in enumerate(test_prompts, 1):
    print(f"[Test {i}] User: {prompt}")
    response = chat(prompt)
    print(f"[Test {i}] G·∫•u: {response}")
    print()

# ============================================
# Interactive chat
# ============================================
print("=" * 60)
print("üí¨ CHAT TR·ª∞C TI·∫æP")
print("=" * 60)
print()
print("B√¢y gi·ªù b·∫°n c√≥ th·ªÉ chat v·ªõi G·∫•u K·∫πo!")
print("(G√µ 'exit' ƒë·ªÉ tho√°t)")
print()

while True:
    user_input = input("You: ").strip()

    if user_input.lower() in ['exit', 'quit', 'bye']:
        print()
        print("G·∫•u: byeee üêß take care nha!")
        break

    if not user_input:
        continue

    response = chat(user_input)
    print(f"G·∫•u: {response}")
    print()
