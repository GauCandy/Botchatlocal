#!/usr/bin/env python3
"""
üî• Train G·∫•u K·∫πo v·ªõi GPU (Local Fine-tuning)
S·ª≠ d·ª•ng Unsloth + LoRA ƒë·ªÉ train model local

Y√™u c·∫ßu:
- GPU NVIDIA (√≠t nh·∫•t 8GB VRAM)
- pip install unsloth transformers datasets bitsandbytes

Ch·∫°y: python train_local_gpu.py
"""

import json
import torch
from pathlib import Path

print("=" * 60)
print("üêß G·∫§U K·∫∏O - LOCAL GPU TRAINING")
print("=" * 60)
print()

# Ki·ªÉm tra GPU
if not torch.cuda.is_available():
    print("‚ùå KH√îNG T√åM TH·∫§Y GPU!")
    print()
    print("C√≥ th·ªÉ do:")
    print("  1. PyTorch CPU-only version (most common)")
    print("  2. NVIDIA drivers ch∆∞a c√†i")
    print("  3. GPU b·ªã t·∫Øt trong BIOS")
    print()
    print("FIX:")
    print("  1. Ch·∫°y: python check_gpu.py")
    print("  2. Xem h∆∞·ªõng d·∫´n trong INSTALL_WINDOWS.md")
    print()
    print("Ho·∫∑c d√πng OpenAI training: python train_openai.py")
    print()
    print("C√≥ mu·ªën ti·∫øp t·ª•c train tr√™n CPU? (R·∫§T CH·∫¨M) (y/n)")
    if input().lower() != 'y':
        exit()
    print()
    print("‚ö†Ô∏è  Training tr√™n CPU... (c√≥ th·ªÉ m·∫•t v√†i gi·ªù)")
else:
    gpu_name = torch.cuda.get_device_name(0)
    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f"‚úÖ GPU detected: {gpu_name}")
    print(f"‚úÖ VRAM: {vram_gb:.1f} GB")

    # Check VRAM
    if vram_gb < 6:
        print()
        print(f"‚ö†Ô∏è  C·∫¢NH B√ÅO: VRAM th·∫•p ({vram_gb:.1f} GB)")
        print("   C√≥ th·ªÉ g·∫∑p l·ªói out of memory")
        print("   Recommend: √≠t nh·∫•t 6GB VRAM")
        print()
        print("Ti·∫øp t·ª•c? (y/n)")
        if input().lower() != 'y':
            exit()
    elif vram_gb < 8:
        print(f"   ‚ö†Ô∏è  VRAM h∆°i th·∫•p ({vram_gb:.1f} GB) - s·∫Ω d√πng batch size nh·ªè")

print()
print("ƒêang c√†i ƒë·∫∑t dependencies...")

try:
    from unsloth import FastLanguageModel
    from datasets import Dataset
    from trl import SFTTrainer
    from transformers import TrainingArguments
except ImportError:
    print("‚ö†Ô∏è  Ch∆∞a c√†i ƒë·∫∑t th∆∞ vi·ªán c·∫ßn thi·∫øt!")
    print("   Ch·∫°y: pip install unsloth transformers datasets trl bitsandbytes")
    print()
    print("B·∫°n c√≥ mu·ªën t·ª± ƒë·ªông c√†i kh√¥ng? (y/n)")
    if input().lower() == 'y':
        import subprocess
        subprocess.run(["pip", "install", "unsloth", "transformers", "datasets", "trl", "bitsandbytes"])
        print("‚úì ƒê√£ c√†i xong! Vui l√≤ng ch·∫°y l·∫°i script.")
    exit()

# ============================================
# B∆Ø·ªöC 1: ƒê·ªçc training data
# ============================================
print()
print("üìñ ƒê·ªçc training data...")

with open('training_data/gau_keo/personality_profile.json', 'r', encoding='utf-8') as f:
    personality = json.load(f)

with open('training_data/gau_keo/conversations.json', 'r', encoding='utf-8') as f:
    conversations = json.load(f)

# Chu·∫©n b·ªã system prompt
system_prompt = f"""B·∫°n l√† {personality['character_name']}.

T√≠nh c√°ch: {personality['communication_style']['tone']}
Emoji hay d√πng: {', '.join(personality['communication_style']['signature_emojis'])}
Style: {personality['communication_style']['language']}

H√£y tr·∫£ l·ªùi nh∆∞ G·∫•u K·∫πo - m·ªÅm m·∫°i, d·ªÖ th∆∞∆°ng, v√† ch√¢n th√†nh."""

# Chuy·ªÉn ƒë·ªïi sang format training
training_data = []
for conv in conversations:
    messages = [{"role": "system", "content": system_prompt}]
    messages.extend(conv['conversation'])

    # Format th√†nh text cho training
    text = ""
    for msg in messages:
        if msg['role'] == 'system':
            text += f"<|system|>\n{msg['content']}\n"
        elif msg['role'] == 'user':
            text += f"<|user|>\n{msg['content']}\n"
        elif msg['role'] == 'assistant':
            text += f"<|assistant|>\n{msg['content']}\n"

    training_data.append({"text": text})

dataset = Dataset.from_list(training_data)
print(f"‚úì Loaded {len(training_data)} conversations")

# ============================================
# B∆Ø·ªöC 2: Load model
# ============================================
print()
print("ü§ñ Loading base model...")
print("   S·ª≠ d·ª•ng: Qwen/Qwen2.5-1.5B (nh·ªè, nhanh)")

max_seq_length = 2048
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Qwen2.5-1.5B-bnb-4bit",  # Model nh·ªè, fit v·ªõi GPU 8GB
    max_seq_length=max_seq_length,
    dtype=None,
    load_in_4bit=True,  # 4-bit quantization ƒë·ªÉ ti·∫øt ki·ªám VRAM
)

print("‚úì Model loaded!")

# ============================================
# B∆Ø·ªöC 3: Setup LoRA
# ============================================
print()
print("‚öôÔ∏è  Setting up LoRA...")

model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # LoRA rank
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
)

print("‚úì LoRA configured!")

# ============================================
# B∆Ø·ªöC 4: Train
# ============================================
print()
print("üî• B·∫Øt ƒë·∫ßu training...")
print("   (C√≥ th·ªÉ m·∫•t 10-30 ph√∫t t√πy GPU)")
print()

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        num_train_epochs=3,  # 3 epochs
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="outputs",
        save_strategy="epoch",
    ),
)

trainer.train()

print()
print("=" * 60)
print("‚úÖ TRAINING HO√ÄN TH√ÄNH!")
print("=" * 60)

# ============================================
# B∆Ø·ªöC 5: L∆∞u model
# ============================================
print()
print("üíæ L∆∞u model...")

output_dir = Path("models/gau_keo_local")
output_dir.mkdir(parents=True, exist_ok=True)

model.save_pretrained(str(output_dir))
tokenizer.save_pretrained(str(output_dir))

print(f"‚úì Model ƒë√£ l∆∞u t·∫°i: {output_dir}")
print()
print("üéØ ƒê·ªÉ test model, ch·∫°y: python test_personality.py --local")
