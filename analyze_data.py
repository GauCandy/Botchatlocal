#!/usr/bin/env python3
"""
Data Analyzer - Ph√¢n t√≠ch d·ªØ li·ªáu training ƒë√£ sinh
Gi√∫p ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng v√† ph√¢n b·ªë d·ªØ li·ªáu
"""

import json
import sys
from pathlib import Path
from collections import Counter
import statistics


def load_data(filepath):
    """Load data t·ª´ JSON file"""
    with open(filepath, 'r', encoding='utf-8') as f:
        if filepath.endswith('.jsonl'):
            # JSONL format
            data = []
            for line in f:
                if line.strip():
                    data.append(json.loads(line))
            return data
        else:
            # JSON format
            return json.load(f)


def analyze_conversations(data):
    """Ph√¢n t√≠ch chi ti·∫øt d·ªØ li·ªáu conversations"""

    if not data:
        print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch!")
        return

    print("=" * 70)
    print("üìä PH√ÇN T√çCH D·ªÆ LI·ªÜU TRAINING")
    print("=" * 70)
    print()

    # Basic stats
    total_convs = len(data)
    print(f"üìà Th·ªëng k√™ c∆° b·∫£n:")
    print(f"  ‚Ä¢ T·ªïng s·ªë conversations: {total_convs}")

    # Turns analysis
    turns_counts = [len(conv.get('conversation', [])) for conv in data]
    if turns_counts:
        print(f"  ‚Ä¢ T·ªïng s·ªë turns: {sum(turns_counts)}")
        print(f"  ‚Ä¢ Trung b√¨nh turns/conv: {statistics.mean(turns_counts):.2f}")
        print(f"  ‚Ä¢ Min turns: {min(turns_counts)}")
        print(f"  ‚Ä¢ Max turns: {max(turns_counts)}")
        print(f"  ‚Ä¢ Median turns: {statistics.median(turns_counts)}")
    print()

    # Length analysis
    print(f"üìè Ph√¢n t√≠ch ƒë·ªô d√†i:")
    all_lengths = []
    user_lengths = []
    assistant_lengths = []

    for conv in data:
        for turn in conv.get('conversation', []):
            length = len(turn.get('content', ''))
            all_lengths.append(length)

            if turn.get('role') == 'user':
                user_lengths.append(length)
            elif turn.get('role') == 'assistant':
                assistant_lengths.append(length)

    if all_lengths:
        print(f"  ‚Ä¢ Trung b√¨nh k√Ω t·ª±/turn: {statistics.mean(all_lengths):.0f}")
        print(f"  ‚Ä¢ Min: {min(all_lengths)} k√Ω t·ª±")
        print(f"  ‚Ä¢ Max: {max(all_lengths)} k√Ω t·ª±")

    if user_lengths and assistant_lengths:
        print(f"  ‚Ä¢ TB User messages: {statistics.mean(user_lengths):.0f} k√Ω t·ª±")
        print(f"  ‚Ä¢ TB Assistant messages: {statistics.mean(assistant_lengths):.0f} k√Ω t·ª±")
    print()

    # Topics distribution
    print(f"üìö Ph√¢n b·ªë theo ch·ªß ƒë·ªÅ:")
    topics = []
    for conv in data:
        topic = conv.get('scenario', {}).get('topic', 'Unknown')
        topics.append(topic)

    topic_counts = Counter(topics)
    for topic, count in topic_counts.most_common(15):
        percentage = (count / total_convs) * 100
        bar = "‚ñà" * int(percentage / 2)
        print(f"  {topic[:40]:40} {count:3} ({percentage:5.1f}%) {bar}")
    print()

    # Categories (if available)
    categories = []
    for conv in data:
        cat = conv.get('scenario', {}).get('category') or conv.get('metadata', {}).get('category')
        if cat:
            categories.append(cat)

    if categories:
        print(f"üè∑Ô∏è  Ph√¢n b·ªë theo category:")
        cat_counts = Counter(categories)
        for cat, count in cat_counts.most_common():
            percentage = (count / len(categories)) * 100
            print(f"  {cat:20} {count:3} ({percentage:5.1f}%)")
        print()

    # Difficulty (if available)
    difficulties = []
    for conv in data:
        diff = conv.get('metadata', {}).get('difficulty')
        if diff:
            difficulties.append(diff)

    if difficulties:
        print(f"‚≠ê Ph√¢n b·ªë ƒë·ªô kh√≥:")
        diff_counts = Counter(difficulties)
        for diff, count in diff_counts.most_common():
            percentage = (count / len(difficulties)) * 100
            print(f"  {diff:10} {count:3} ({percentage:5.1f}%)")
        print()

    # Language distribution
    languages = []
    for conv in data:
        lang = conv.get('metadata', {}).get('language', 'unknown')
        languages.append(lang)

    if languages:
        print(f"üåê Ng√¥n ng·ªØ:")
        lang_counts = Counter(languages)
        for lang, count in lang_counts.most_common():
            percentage = (count / total_convs) * 100
            print(f"  {lang:10} {count:3} ({percentage:5.1f}%)")
        print()

    # Source models
    sources = []
    for conv in data:
        source = conv.get('source', 'unknown')
        sources.append(source)

    if sources:
        print(f"ü§ñ Models s·ª≠ d·ª•ng:")
        source_counts = Counter(sources)
        for source, count in source_counts.most_common():
            percentage = (count / total_convs) * 100
            print(f"  {source:30} {count:3} ({percentage:5.1f}%)")
        print()

    # Quality metrics
    print(f"‚úÖ Ch·∫•t l∆∞·ª£ng:")

    # Check for very short responses
    short_responses = sum(1 for length in all_lengths if length < 20)
    if all_lengths:
        print(f"  ‚Ä¢ Responses qu√° ng·∫Øn (<20 chars): {short_responses} ({short_responses/len(all_lengths)*100:.1f}%)")

    # Check for very long responses
    long_responses = sum(1 for length in all_lengths if length > 1000)
    if all_lengths:
        print(f"  ‚Ä¢ Responses qu√° d√†i (>1000 chars): {long_responses} ({long_responses/len(all_lengths)*100:.1f}%)")

    # Check for conversations with odd number of turns (might be incomplete)
    odd_turns = sum(1 for count in turns_counts if count % 2 != 0)
    print(f"  ‚Ä¢ Conversations v·ªõi l∆∞·ª£t l·∫ª: {odd_turns} ({odd_turns/total_convs*100:.1f}%)")

    print()

    # Recommendations
    print("=" * 70)
    print("üí° KHUY·∫æN NGH·ªä")
    print("=" * 70)

    issues = []

    if statistics.mean(turns_counts) < 4:
        issues.append("‚ö†Ô∏è  Conversations h∆°i ng·∫Øn. TƒÉng DEFAULT_TURNS trong config.py")

    if short_responses > len(all_lengths) * 0.1:  # >10% responses ng·∫Øn
        issues.append("‚ö†Ô∏è  Nhi·ªÅu responses qu√° ng·∫Øn. TƒÉng MIN_RESPONSE_LENGTH")

    if len(topic_counts) < 5:
        issues.append("‚ö†Ô∏è  Thi·∫øu ƒëa d·∫°ng ch·ªß ƒë·ªÅ. Th√™m scenarios m·ªõi")

    if len(set(topics)) < total_convs * 0.3:  # <30% unique topics
        issues.append("‚ö†Ô∏è  Nhi·ªÅu ch·ªß ƒë·ªÅ b·ªã duplicate. Th√™m scenarios ƒëa d·∫°ng h∆°n")

    if issues:
        for issue in issues:
            print(f"  {issue}")
    else:
        print("  ‚úÖ D·ªØ li·ªáu c√≥ ch·∫•t l∆∞·ª£ng t·ªët!")
        print("  ‚úÖ Ph√¢n b·ªë ƒëa d·∫°ng v√† c√¢n b·∫±ng!")

    print()
    print("=" * 70)


def main():
    """Main function"""

    if len(sys.argv) < 2:
        # T·ª± ƒë·ªông t√¨m file m·ªõi nh·∫•t trong training_data/
        data_dir = Path("training_data")
        if not data_dir.exists():
            print("‚ùå Th∆∞ m·ª•c training_data kh√¥ng t·ªìn t·∫°i!")
            print()
            print("S·ª≠ d·ª•ng: python analyze_data.py <path_to_json_file>")
            return

        # T√¨m file .json m·ªõi nh·∫•t
        json_files = list(data_dir.glob("*.json"))
        if not json_files:
            print("‚ùå Kh√¥ng t√¨m th·∫•y file JSON n√†o trong training_data/")
            print()
            print("S·ª≠ d·ª•ng: python analyze_data.py <path_to_json_file>")
            return

        # S·∫Øp x·∫øp theo th·ªùi gian, l·∫•y file m·ªõi nh·∫•t
        filepath = max(json_files, key=lambda p: p.stat().st_mtime)
        print(f"üìÇ T·ª± ƒë·ªông ph√°t hi·ªán file: {filepath}")
        print()

    else:
        filepath = Path(sys.argv[1])

    if not filepath.exists():
        print(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {filepath}")
        return

    # Load and analyze
    try:
        print(f"üìñ ƒêang ƒë·ªçc file: {filepath}")
        print()
        data = load_data(filepath)
        analyze_conversations(data)

    except Exception as e:
        print(f"‚ùå L·ªói khi ph√¢n t√≠ch: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
